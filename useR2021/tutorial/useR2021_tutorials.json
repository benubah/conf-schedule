[
  {
    "Id": "35b529310e3ba8ed116e233cabb47d22",
    "resourceId": "Track 1",
    "title": "Data visualization using ggplot2 and its extensions",
    "language": "English",
    "instructors": "Messaoud, Haifa Ben;\nBelaid, Mouna;\nDriss, Kaouthar;\nSouissi, Amir",
    "duration": 125,
    "attendees": 100,
    "level": "Beginner",
    "start": "2021-07-07T07:00:00Z",
    "end": "2021-07-07T09:00:00Z",
    "summary": "This tutorial will cover the introduction to ggplot2 and its \nmain functions. We will cover how to make visualization of one variable, two \nvariables, and three or more variables, how to lay out multiple plots, \nthe use of ggstats for statistical visualizations, how to make \ninteractive graphs using plotly and gganimate, some extensions of \nggplot2.Finally, we will show you how to enhance the \nquality of your graphs by changing the theme or adding a logo and how \nto export your graph. We will share the code on the github repository of R-Ladies \nTunis.",
    "description": "<b><u>Title<\/u><\/b> <br> Data visualization using ggplot2 and its extensions <br><br><b><u>Summary<\/u><\/b> <br> This tutorial will cover the introduction to ggplot2 and its \nmain functions. We will cover how to make visualization of one variable, two \nvariables, and three or more variables, how to lay out multiple plots, \nthe use of ggstats for statistical visualizations, how to make \ninteractive graphs using plotly and gganimate, some extensions of \nggplot2.Finally, we will show you how to enhance the \nquality of your graphs by changing the theme or adding a logo and how \nto export your graph. We will share the code on the github repository of R-Ladies \nTunis. <br><br><b><u>Instructors: <\/u><\/b> Messaoud, Haifa Ben;\nBelaid, Mouna;\nDriss, Kaouthar;\nSouissi, Amir <br><b><u>Language: <\/u><\/b> English <br><b><u>Level: <\/u><\/b> Beginner"
  },
  {
    "Id": "2ca367f20b0edab3db1166fa9d1ac34e",
    "resourceId": "Track 1",
    "title": "Additive Bayesian Networks Modeling",
    "language": "English",
    "instructors": "Kratzer, Gilles;\nFurrer, Reinhard",
    "duration": 150,
    "attendees": 60,
    "level": "Intermediate",
    "start": "2021-07-07T09:15:00Z",
    "end": "2021-07-07T11:45:00Z",
    "summary": "Additive Bayesian Networks (ABN) have been developed to disentangle \ncomplex relationships of highly correlated datasets as frequently \nencountered in risk factor analysis studies. ABN is an efficient \napproach to sort out direct and indirect relationships among variables \nwhich is surprisingly common in systemic epidemiology. After the \ntutorial, you will be able to run the particular steps within \nan ABN analysis with real-world data. You will be able to contrast this \napproach with standard regression (linear, logistic, Poisson regression,\n and multinomial models) used for classical risk factor analysis. \nTowards the end, we also cover Bayesian Model Averaging in the context \nof an ABN which is useful to assess the validity of the learned model as\n well as for more advanced inference on the network.",
    "description": "<b><u>Title<\/u><\/b> <br> Additive Bayesian Networks Modeling <br><br><b><u>Summary<\/u><\/b> <br> Additive Bayesian Networks (ABN) have been developed to disentangle \ncomplex relationships of highly correlated datasets as frequently \nencountered in risk factor analysis studies. ABN is an efficient \napproach to sort out direct and indirect relationships among variables \nwhich is surprisingly common in systemic epidemiology. After the \ntutorial, you will be able to run the particular steps within \nan ABN analysis with real-world data. You will be able to contrast this \napproach with standard regression (linear, logistic, Poisson regression,\n and multinomial models) used for classical risk factor analysis. \nTowards the end, we also cover Bayesian Model Averaging in the context \nof an ABN which is useful to assess the validity of the learned model as\n well as for more advanced inference on the network. <br><br><b><u>Instructors: <\/u><\/b> Kratzer, Gilles;\nFurrer, Reinhard <br><b><u>Language: <\/u><\/b> English <br><b><u>Level: <\/u><\/b> Intermediate"
  },
  {
    "Id": "4d20a8fa8e9fcff5d226c8ebb6f7c29e",
    "resourceId": "Track 1",
    "title": "Quick high quality maps with R",
    "language": "English",
    "instructors": "Kolb, Jan-Philipp",
    "duration": 120,
    "attendees": 40,
    "level": "Beginner",
    "start": "2021-07-07T12:00:00Z",
    "end": "2021-07-07T14:00:00Z",
    "summary": "This tutorial covers the basic use of R for creating maps. Useful tools as well as data sources are both presented.  Concerning tools, the focus is on the packages osmplotr, \ntmap and raster. \nIn the first part of the tutorial, you will learn how to use Openstreetmap data. Geocoding and creation of bounding boxes will be presented as \nwell as the use of shapefiles to create thematic maps and color coding in R. After this introduction to the basic concepts and functionalities of \nmapping with R, you will go through a prototypical data analysis workflow: import, wrangling, exploration, (basic) analysis, \nreporting. You will  have the opportunity to create your own maps during \nthe workshop. A github repo on the course will be shared.",
    "description": "<b><u>Title<\/u><\/b> <br> Quick high quality maps with R <br><br><b><u>Summary<\/u><\/b> <br> This tutorial covers the basic use of R for creating maps. Useful tools as well as data sources are both presented.  Concerning tools, the focus is on the packages osmplotr, \ntmap and raster. \nIn the first part of the tutorial, you will learn how to use Openstreetmap data. Geocoding and creation of bounding boxes will be presented as \nwell as the use of shapefiles to create thematic maps and color coding in R. After this introduction to the basic concepts and functionalities of \nmapping with R, you will go through a prototypical data analysis workflow: import, wrangling, exploration, (basic) analysis, \nreporting. You will  have the opportunity to create your own maps during \nthe workshop. A github repo on the course will be shared. <br><br><b><u>Instructors: <\/u><\/b> Kolb, Jan-Philipp <br><b><u>Language: <\/u><\/b> English <br><b><u>Level: <\/u><\/b> Beginner"
  },
  {
    "Id": "9acd29aaa7195b287500f887f6cd1f25",
    "resourceId": "Track 1",
    "title": "Spatial Statistics for huge datasets and best practices",
    "language": "English",
    "instructors": "Furrer, Reinhard;\nFlury, Roman;\nBlasi, Federico",
    "duration": 120,
    "attendees": 50,
    "level": "Advanced",
    "start": "2021-07-07T14:15:00Z",
    "end": "2021-07-07T16:15:00Z",
    "summary": "During the last decade, several advanced approaches have \nbeen proposed to address computational issues of larger and larger multivariate space-time \ndatasets . These can essentially be \ncategorized as (i) construct \"simpler\" models or (e.g., low-rank models,\n composite likelihood methods, predictive process models) (ii) \napproximate the models (e.g., with Gaussian Markov random fields, \ncompactly supported covariance function). In this tutorial, we discuss \nthis last point by using sparse covariance matrix approximations. There \nis seemingly no limit to the sample size with the possibility of working\n with long vectors jointly with 64bit handling algorithms. However, the \ndevil is in the details and to avoid encountering negative surprises we \nprovide best practices, strategies, and tricks when modeling huge \nspatial data.",
    "description": "<b><u>Title<\/u><\/b> <br> Spatial Statistics for huge datasets and best practices <br><br><b><u>Summary<\/u><\/b> <br> During the last decade, several advanced approaches have \nbeen proposed to address computational issues of larger and larger multivariate space-time \ndatasets . These can essentially be \ncategorized as (i) construct \"simpler\" models or (e.g., low-rank models,\n composite likelihood methods, predictive process models) (ii) \napproximate the models (e.g., with Gaussian Markov random fields, \ncompactly supported covariance function). In this tutorial, we discuss \nthis last point by using sparse covariance matrix approximations. There \nis seemingly no limit to the sample size with the possibility of working\n with long vectors jointly with 64bit handling algorithms. However, the \ndevil is in the details and to avoid encountering negative surprises we \nprovide best practices, strategies, and tricks when modeling huge \nspatial data. <br><br><b><u>Instructors: <\/u><\/b> Furrer, Reinhard;\nFlury, Roman;\nBlasi, Federico <br><b><u>Language: <\/u><\/b> English <br><b><u>Level: <\/u><\/b> Advanced"
  },
  {
    "Id": "X2",
    "resourceId": "Track 1",
    "title": "Contributing to R",
    "language": "English",
    "instructors": "Gabriel Becker; Martin Maechler",
    "duration": 180,
    "attendees": 30,
    "level": "Intermediate",
    "start": "2021-07-07T16:30:00Z",
    "end": "2021-07-07T19:30:00Z",
    "summary": "Did you always want to contribute to (base) R but don't know how? Come to our Tutorial!\nWe will show cases where and how users have \ncontributed actively to (base) R, by submitting bug reports with minimal\n reproducible examples, how testing, reading source code, and providing \npatches to the R source code has helped making R better.\nDepending on the participants willingness and \nlevel of sophistication, we will look into doing things right now, for \ncurrently non-resolved issues and bug reports.",
    "description": "<b><u>Title<\/u><\/b> <br> Contributing to R <br><br><b><u>Summary<\/u><\/b> <br> Did you always want to contribute to (base) R but don't know how? Come to our Tutorial!\nWe will show cases where and how users have \ncontributed actively to (base) R, by submitting bug reports with minimal\n reproducible examples, how testing, reading source code, and providing \npatches to the R source code has helped making R better.\nDepending on the participants willingness and \nlevel of sophistication, we will look into doing things right now, for \ncurrently non-resolved issues and bug reports. <br><br><b><u>Instructors: <\/u><\/b> Gabriel Becker; Martin Maechler <br><b><u>Language: <\/u><\/b> English <br><b><u>Level: <\/u><\/b> Intermediate"
  },
  {
    "Id": "2b9ee3e3cd67265457a88d68feb35bcc",
    "resourceId": "Track 1",
    "title": "Graphing multivariate categorical data: The how, what and why of mosaic plots and alluvial diagrams",
    "language": "English",
    "instructors": "Robbins, Joyce;\nJanda, Ludmila",
    "duration": 150,
    "attendees": 24,
    "level": "Beginner",
    "start": "2021-07-07T19:45:00Z",
    "end": "2021-07-07T22:15:00Z",
    "summary": "Multivariate\n categorical data present unique data visualization challenges. This \ntutorial provides two options to meet such challenges: mosaic plots and \nalluvial diagrams. First, we will focus on how to choose the best graph \nfor given data types and communication goals. You will then \nlearn how to get the underlying data in the correct shape to make each \ngraph and then create both graph types using the vcd and ggalluvial \npackages. We will use engaging datasets and aim to equip you\nwith the skills to make these graphs (and the choice whether to use \nthem) on your own.",
    "description": "<b><u>Title<\/u><\/b> <br> Graphing multivariate categorical data: The how, what and why of mosaic plots and alluvial diagrams <br><br><b><u>Summary<\/u><\/b> <br> Multivariate\n categorical data present unique data visualization challenges. This \ntutorial provides two options to meet such challenges: mosaic plots and \nalluvial diagrams. First, we will focus on how to choose the best graph \nfor given data types and communication goals. You will then \nlearn how to get the underlying data in the correct shape to make each \ngraph and then create both graph types using the vcd and ggalluvial \npackages. We will use engaging datasets and aim to equip you\nwith the skills to make these graphs (and the choice whether to use \nthem) on your own. <br><br><b><u>Instructors: <\/u><\/b> Robbins, Joyce;\nJanda, Ludmila <br><b><u>Language: <\/u><\/b> English <br><b><u>Level: <\/u><\/b> Beginner"
  },
  {
    "Id": "1c362cac708b57bc91214a160fb9f0aa",
    "resourceId": "Track 4",
    "title": "Structure your app: introduction to Shiny modules",
    "language": "English",
    "instructors": "Hagenberg, Jonas",
    "duration": 120,
    "attendees": 25,
    "level": "Intermediate",
    "start": "2021-07-07T07:00:00Z",
    "end": "2021-07-07T09:00:00Z",
    "summary": "You communicate your results interactively \nwith Shiny, maintain a dashboard or provide business logic, but the \ncodebase of your app becomes too complex? Then modules are the right \ntool for you, they are the Shiny built-in solution to manage this \ncomplexity. Shiny modules allow you to break down your code into smaller\n building blocks that can be combined and reused.\nIn this tutorial I give an introduction into \nmodules, its advantages over simple R functions and how existing \nfunctionality can be transferred to modules.\nFor an easy start, I cover common pitfalls needed to overcome for a productive use of modules:\n- Passing reactive objects to modules\n- Returning reactive values from the module to the calling environment\n- Nesting modules\n- Dynamically generating modules (including UI)\nThe contents of the tutorial are delivered by \nshort lectures followed by hands-on coding sessions in break-out rooms. \nFor this, you need a basic knowledge of reactive programming/Shiny.",
    "description": "<b><u>Title<\/u><\/b> <br> Structure your app: introduction to Shiny module <br><br><b><u>Summary<\/u><\/b> <br> You communicate your results interactively \nwith Shiny, maintain a dashboard or provide business logic, but the \ncodebase of your app becomes too complex? Then modules are the right \ntool for you, they are the Shiny built-in solution to manage this \ncomplexity. Shiny modules allow you to break down your code into smaller\n building blocks that can be combined and reused.\nIn this tutorial I give an introduction into \nmodules, its advantages over simple R functions and how existing \nfunctionality can be transferred to modules.\nFor an easy start, I cover common pitfalls needed to overcome for a productive use of modules:\n- Passing reactive objects to modules\n- Returning reactive values from the module to the calling environment\n- Nesting modules\n- Dynamically generating modules (including UI)\nThe contents of the tutorial are delivered by \nshort lectures followed by hands-on coding sessions in break-out rooms. \nFor this, you need a basic knowledge of reactive programming/Shiny. <br><br><b><u>Instructors: <\/u><\/b> Hagenberg, Jonas <br><b><u>Language: <\/u><\/b> English <br><b><u>Level: <\/u><\/b> Intermediate"
  },
  {
    "Id": "73ed93cf6651075a3af3788b47a747de",
    "resourceId": "Track 4",
    "title": "Getting started with torch",
    "language": "French",
    "instructors": "Keydana, Sigrid",
    "duration": 180,
    "attendees": 100,
    "level": "Intermediate",
    "start": "2021-07-07T10:30:00Z",
    "end": "2021-07-07T13:30:00Z",
    "summary": "Torch (https://torch.mlverse.org/) is an open source machine learning \nframework based on PyTorch. Not requiring any Python dependencies, torch\n for R is at once a powerful computational engine with including GPU \nacceleration, a neural network library, and an ecosystem providing tools\n for, among others, image, text, and audio processing. This tutorial \nwill provide a thorough introduction to torch basics: tensors, automatic\n differentiation, and neural network modules. Thereafter, we delve into \ntwo areas of special interest to R users: time series forecasting and \nnumerical optimization. All sections will include time slots for \npractice.",
    "description": "<b><u>Title<\/u><\/b> <br> Getting started with torch <br><br><b><u>Summary<\/u><\/b> <br> Torch (https://torch.mlverse.org/) is an open source machine learning \nframework based on PyTorch. Not requiring any Python dependencies, torch\n for R is at once a powerful computational engine with including GPU \nacceleration, a neural network library, and an ecosystem providing tools\n for, among others, image, text, and audio processing. This tutorial \nwill provide a thorough introduction to torch basics: tensors, automatic\n differentiation, and neural network modules. Thereafter, we delve into \ntwo areas of special interest to R users: time series forecasting and \nnumerical optimization. All sections will include time slots for \npractice. <br><br><b><u>Instructors: <\/u><\/b> Keydana, Sigrid <br><b><u>Language: <\/u><\/b> French <br><b><u>Level: <\/u><\/b> Intermediate"
  },
  {
    "Id": "45dfd491842988c49f39ce7d84167bd9",
    "resourceId": "Track 4",
    "title": "Pinguinos en caja : tutorial interactivo de ciencia de datos con pinguinos.",
    "language": "Spanish",
    "instructors": "Dermit, Maria;\nEscobar, Susana",
    "duration": 60,
    "attendees": 100,
    "level": "Intermediate",
    "start": "2021-07-07T13:45:00Z",
    "end": "2021-07-07T14:45:00Z",
    "summary": "Pingüinos en Caja es un paquete learnr que cubre los temas del libro R para ciencia de datos y utiliza el conocido paquete de datos pinguinos para explorar los conceptos del libro. \nEl paquete contiene actualmente un tutorial para cada capítulo del libro y se presentará durante el taller. Además, los asistentes trabajarán en salas para grupos pequeños\nmódulos divididos por las secciones principales del libro (por ejemplo, Explorar, Wrangle, Programar, Modelar y Comunicar; 6 apartados en total) según sus objetivos de aprendizaje. \nLas personas de la audiencia de este tutorial son estudiantes que quieran mejorar sus habilidades en ciencia de datos de forma interactiva y docentes que quieran acceder a recursos de aprendizaje adicionales similares a\nlos Primers de Rstudio (https://rstudio.cloud/learn/primers). El tutorial tiene como objetivo ser interactivo y la instrucción entre pares entre los asistentes será dirigida para guiar el aprendizaje en las salas de grupos pequeños.",
    "description": "<b><u>Title<\/u><\/b> <br> Pinguinos en caja : tutorial interactivo de ciencia de datos con pinguinos. <br><br><b><u>Summary<\/u><\/b> <br> Pingüinos en Caja es un paquete learnr que cubre los temas del libro R para ciencia de datos y utiliza el conocido paquete de datos pinguinos para explorar los conceptos del libro. \nEl paquete contiene actualmente un tutorial para cada capítulo del libro y se presentará durante el taller. Además, los asistentes trabajarán en salas para grupos pequeños\nmódulos divididos por las secciones principales del libro (por ejemplo, Explorar, Wrangle, Programar, Modelar y Comunicar; 6 apartados en total) según sus objetivos de aprendizaje. \nLas personas de la audiencia de este tutorial son estudiantes que quieran mejorar sus habilidades en ciencia de datos de forma interactiva y docentes que quieran acceder a recursos de aprendizaje adicionales similares a\nlos Primers de Rstudio (https://rstudio.cloud/learn/primers). El tutorial tiene como objetivo ser interactivo y la instrucción entre pares entre los asistentes será dirigida para guiar el aprendizaje en las salas de grupos pequeños. <br><br><b><u>Instructors: <\/u><\/b> Dermit, Maria;\nEscobar, Susana <br><b><u>Language: <\/u><\/b> Spanish <br><b><u>Level: <\/u><\/b> Intermediate"
  },
  {
    "Id": "15b5e9a477b3065990bc00a026602a0e",
    "resourceId": "Track 4",
    "title": "Data Pipelines at scale with R and Kubernetes",
    "language": "Spanish",
    "instructors": "van Dunné, Frans",
    "duration": 180,
    "attendees": 40,
    "level": "Advanced",
    "start": "2021-07-07T15:00:00Z",
    "end": "2021-07-07T18:00:00Z",
    "summary": "Many R users are confronted with larger and larger amounts of data that \nneed to be processed. In this tutorial we will show you how to go to \nthe next level by massively parallelizing your R code on a kubernetes \ncluster. We will show you how to move your entire data pipeline to \nKubernetes where each node in the pipeline consists of a container \nrunning R code. These containers can run with multiple cores, and then \nfarmed out to tens or hundreds of these containers running in parallel. \nOur experience has shown that this allows for massive speed gains, at \nrelatively low cost when the kubernetes cluster is populated with \nephemeral virtual machines (e.g. preemptible VM's on GCP - Spot \ninstances on AWS). You need to have an interest in the more\n technical aspects of running R code, but only to a degree. We hope to \ndispel any fear that you might have that setting up a cluster is \nsomething that is very difficult. A \nkey tool we will introduce is a tool to create data pipelines on \nkubernetes called Pachyderm (the open source version). The tutorial will be a combination of theory, break outs \nto run things hands on, regrouping to talk about experiences and then \ntaking the next step. We will set up code examples in steps, so that if \none step di not work out, after regrouping the group can take off from \nthe next starting point.",
    "description": "<b><u>Title<\/u><\/b> <br> Data Pipelines at scale with R and Kubernetes <br><br><b><u>Summary<\/u><\/b> <br> Many R users are confronted with larger and larger amounts of data that \nneed to be processed. In this tutorial we will show you how to go to \nthe next level by massively parallelizing your R code on a kubernetes \ncluster. We will show you how to move your entire data pipeline to \nKubernetes where each node in the pipeline consists of a container \nrunning R code. These containers can run with multiple cores, and then \nfarmed out to tens or hundreds of these containers running in parallel. \nOur experience has shown that this allows for massive speed gains, at \nrelatively low cost when the kubernetes cluster is populated with \nephemeral virtual machines (e.g. preemptible VM's on GCP - Spot \ninstances on AWS). You need to have an interest in the more\n technical aspects of running R code, but only to a degree. We hope to \ndispel any fear that you might have that setting up a cluster is \nsomething that is very difficult. A \nkey tool we will introduce is a tool to create data pipelines on \nkubernetes called Pachyderm (the open source version). The tutorial will be a combination of theory, break outs \nto run things hands on, regrouping to talk about experiences and then \ntaking the next step. We will set up code examples in steps, so that if \none step di not work out, after regrouping the group can take off from \nthe next starting point. <br><br><b><u>Instructors: <\/u><\/b> van Dunné, Frans <br><b><u>Language: <\/u><\/b> Spanish <br><b><u>Level: <\/u><\/b> Advanced"
  },
  {
    "Id": "8e8f09fb84aacae5d87c6b1538c27cd8",
    "resourceId": "Track 4",
    "title": "Tidying up spatial data",
    "language": "Spanish",
    "instructors": "Campitelli, Elio;\nCorrales, Paola",
    "duration": 180,
    "attendees": 40,
    "level": "Intermediate",
    "start": "2021-07-07T18:15:00Z",
    "end": "2021-07-07T21:15:00Z",
    "summary": "En este tutorial vas a aprender a descargar, \nleer, analizar y visualizar datos espaciales grillados en R usando datos\n tidy. Va a ser un tutorial participativo con programación en vivo y \nejercicios, bajo la idea de que puedas usar los datos para responder tus\n propias preguntas, escribiendo tu propio código.\nAl final del taller vas a haber aprendido como:\n- descargar datos meteorológicos y climáticos programáticamente desde R,\n- leerlos en un formato tidy,\n- computar estadísicas espaciales y temporales,\n- graficar los resultados usando ggplot2 y extensiones.",
    "description": "<b><u>Title<\/u><\/b> <br> Tidying up spatial data <br><br><b><u>Summary<\/u><\/b> <br> En este tutorial vas a aprender a descargar, \nleer, analizar y visualizar datos espaciales grillados en R usando datos\n tidy. Va a ser un tutorial participativo con programación en vivo y \nejercicios, bajo la idea de que puedas usar los datos para responder tus\n propias preguntas, escribiendo tu propio código.\nAl final del taller vas a haber aprendido como:\n- descargar datos meteorológicos y climáticos programáticamente desde R,\n- leerlos en un formato tidy,\n- computar estadísicas espaciales y temporales,\n- graficar los resultados usando ggplot2 y extensiones. <br><br><b><u>Instructors: <\/u><\/b> Campitelli, Elio;\nCorrales, Paola <br><b><u>Language: <\/u><\/b> Spanish <br><b><u>Level: <\/u><\/b> Intermediate"
  },
  {
    "Id": "92decf2b834d9b748d155f04391a17e5",
    "resourceId": "Track 3",
    "title": "Introduction to Responsible Machine Learning",
    "language": "English",
    "instructors": "Kozak, Anna;\nBaniecki, Hubert;\nBiecek, Przemyslaw;\nWisniewski, Jakub",
    "duration": 180,
    "attendees": 150,
    "level": "Beginner",
    "start": "2021-07-07T07:00:00Z",
    "end": "2021-07-07T10:00:00Z",
    "summary": "What? \nThe workshop focuses on responsible \nmachine learning, including areas such as model fairness, \nexplainability, and validation.\nWhy? \nTo gain the theory and hands-on experience in developing safe and effective predictive models.\nFor whom? \nFor those with basic knowledge of R, familiar with supervised machine learning and interested in model validation.\nWhat will be used? \nWe will use the DALEX \npackage for explanations, fairmodels for checking bias, and modelStudio \nfor interactive model analysis.\n",
    "description": "<b><u>Title<\/u><\/b> <br> Introduction to Responsible Machine Learning <br><br><b><u>Summary<\/u><\/b> <br> What? \nThe workshop focuses on responsible \nmachine learning, including areas such as model fairness, \nexplainability, and validation.\nWhy? \nTo gain the theory and hands-on experience in developing safe and effective predictive models.\nFor whom? \nFor those with basic knowledge of R, familiar with supervised machine learning and interested in model validation.\nWhat will be used? \nWe will use the DALEX \npackage for explanations, fairmodels for checking bias, and modelStudio \nfor interactive model analysis.\n <br><br><b><u>Instructors: <\/u><\/b> Kozak, Anna;\nBaniecki, Hubert;\nBiecek, Przemyslaw;\nWisniewski, Jakub <br><b><u>Language: <\/u><\/b> English <br><b><u>Level: <\/u><\/b> Beginner"
  },
  {
    "Id": "4301769836f32c8ab956ac9ad9790434",
    "resourceId": "Track 3",
    "title": "Entry level R maps from African data",
    "language": "French - English",
    "instructors": "South, Andy;\nvan der Walt, Anelda;\nDicko, Ahmadou;\nKariuki, Shelmith;\nBaker, Laurie",
    "duration": 240,
    "attendees": 60,
    "level": "Beginner",
    "start": "2021-07-07T10:15:00Z",
    "end": "2021-07-07T14:15:00Z",
    "summary": "This tutorial will provide an introduction to mapping and spatial data in R using \nAfrican data. By the end of the tutorial, you should be \nable to make a map that is useful to you from data that you have \nbrought yourselves. We will focus on developing confidence in \ndoing the basics really well in preference to straying too far into more\n advanced analyses. Our \ntutorials focus on flexible workflows that you can take away. You will also learn how \nto spot and avoid common pitfalls. The training will be \npartly based around a set of interactive learnr tutorials that we have \ncreated as part of the afrilearnr package \n(https://github.com/afrimapr/afrilearnr) and accompanying online demos \ndescribed in this blog post : \nhttps://afrimapr.github.io/afrimapr.website/blog/2021/interactive-tutorials-for-african-maps/.\nThe tutorial will be available on shinyapps for those that are \nunable to install locally. There will be separate English & French language groups \nwith dedicated materials. Each group will start together for the first \nfew sessions and then break into sub-groups\n of up to 10 learners with one trainer each for improved feedback and \ndiscussion. Towards the end of the tutorial we will challenge you \nto make a map using data that you have brought or found. \nEach language group will come back together for a final wrapup session. \n",
    "description": "<b><u>Title<\/u><\/b> <br> Entry level R maps from African data <br><br><b><u>Summary<\/u><\/b> <br> This tutorial will provide an introduction to mapping and spatial data in R using \nAfrican data. By the end of the tutorial, you should be \nable to make a map that is useful to you from data that you have \nbrought yourselves. We will focus on developing confidence in \ndoing the basics really well in preference to straying too far into more\n advanced analyses. Our \ntutorials focus on flexible workflows that you can take away. You will also learn how \nto spot and avoid common pitfalls. The training will be \npartly based around a set of interactive learnr tutorials that we have \ncreated as part of the afrilearnr package \n(https://github.com/afrimapr/afrilearnr) and accompanying online demos \ndescribed in this blog post : \nhttps://afrimapr.github.io/afrimapr.website/blog/2021/interactive-tutorials-for-african-maps/.\nThe tutorial will be available on shinyapps for those that are \nunable to install locally. There will be separate English & French language groups \nwith dedicated materials. Each group will start together for the first \nfew sessions and then break into sub-groups\n of up to 10 learners with one trainer each for improved feedback and \ndiscussion. Towards the end of the tutorial we will challenge you \nto make a map using data that you have brought or found. \nEach language group will come back together for a final wrapup session. \n <br><br><b><u>Instructors: <\/u><\/b> South, Andy;\nvan der Walt, Anelda;\nDicko, Ahmadou;\nKariuki, Shelmith;\nBaker, Laurie <br><b><u>Language: <\/u><\/b> French - English <br><b><u>Level: <\/u><\/b> Beginner"
  },
  {
    "Id": "8eb3232715abf81fb9cd6f36d73e1c57",
    "resourceId": "Track 3",
    "title": "How to build a package with \"Rmd First\" method",
    "language": "English",
    "instructors": "Rochette, Sébastien;\nRiederer, Emily",
    "duration": 180,
    "attendees": 30,
    "level": "Intermediate",
    "start": "2021-07-07T14:30:00Z",
    "end": "2021-07-07T17:30:00Z",
    "summary": "\"Rmd First\" method can reduce mental load when building packages by \nkeeping users in a natural environment, using a tool they know: a \nRMarkdown document. The step between writing your own R code to analyze \nsome data and refactoring it into a well-documented, ready-to-share R \npackage seems unreachable to many R users. The package structure is \nsometimes perceived as useful only for building general-purpose tools \nfor data analysis to be shared on official platforms. However, packages \ncan be used for a broader range of purposes, from internal use to \nopen-source sharing. Because packages are designed for robustness and \nenforce helpful standards for documentation and testing, the package \nstructure provides a useful framework for refactoring analyses and \npreparing them to go into production. The following approach to write a \ndevelopment or an analysis inside a Rmd, will significantly reduce the \nwork to transform a Rmd into a package : - _Design_ : define the goal of\n your next steps and the tools needed to reach them - _Prototype_ : use \nsome small examples to prototype your script in Rmd - _Build_ : Build \nyour script as functions and document your work to be able to use them, \nin the future, on real-life datasets - _Strengthen_ : Create tests to \nassure stability of your code and follow modifications through time - \n_Deploy_ : Transform as a well-structured package to deploy and share \nwith your community During this tutorial, we will work through the steps\n of Rmd Driven Development to persuade attendees that their experience \nwriting R code means that they already know how to build a package. They\n only need to be in a safe environment to find it out, which will be \nwhat we propose. We will take advantage of all existing tools such as \n{devtools}, {testthat}, {attachment} and {usethis} that ease package \ndevelopment from Rmd to building a package. The recent package \n[{fusen}](https://thinkr-open.github.io/fusen), which \"inflates a \npackage from a simple flat Rmd\", will be presented to further reduce the\n step between well-designed Rmd and package deployment. Attendees will \nleave this workshop having built their first package with the \"Rmd \nFirst\" method and with the skills and tools to build more packages on \ntheir own.",
    "description": "<b><u>Title<\/u><\/b> <br> How to build a package with \"Rmd First\" method <br><br><b><u>Summary<\/u><\/b> <br> \"Rmd First\" method can reduce mental load when building packages by \nkeeping users in a natural environment, using a tool they know: a \nRMarkdown document. The step between writing your own R code to analyze \nsome data and refactoring it into a well-documented, ready-to-share R \npackage seems unreachable to many R users. The package structure is \nsometimes perceived as useful only for building general-purpose tools \nfor data analysis to be shared on official platforms. However, packages \ncan be used for a broader range of purposes, from internal use to \nopen-source sharing. Because packages are designed for robustness and \nenforce helpful standards for documentation and testing, the package \nstructure provides a useful framework for refactoring analyses and \npreparing them to go into production. The following approach to write a \ndevelopment or an analysis inside a Rmd, will significantly reduce the \nwork to transform a Rmd into a package : - _Design_ : define the goal of\n your next steps and the tools needed to reach them - _Prototype_ : use \nsome small examples to prototype your script in Rmd - _Build_ : Build \nyour script as functions and document your work to be able to use them, \nin the future, on real-life datasets - _Strengthen_ : Create tests to \nassure stability of your code and follow modifications through time - \n_Deploy_ : Transform as a well-structured package to deploy and share \nwith your community During this tutorial, we will work through the steps\n of Rmd Driven Development to persuade attendees that their experience \nwriting R code means that they already know how to build a package. They\n only need to be in a safe environment to find it out, which will be \nwhat we propose. We will take advantage of all existing tools such as \n{devtools}, {testthat}, {attachment} and {usethis} that ease package \ndevelopment from Rmd to building a package. The recent package \n[{fusen}](https://thinkr-open.github.io/fusen), which \"inflates a \npackage from a simple flat Rmd\", will be presented to further reduce the\n step between well-designed Rmd and package deployment. Attendees will \nleave this workshop having built their first package with the \"Rmd \nFirst\" method and with the skills and tools to build more packages on \ntheir own. <br><br><b><u>Instructors: <\/u><\/b> Rochette, Sébastien;\nRiederer, Emily <br><b><u>Language: <\/u><\/b> English <br><b><u>Level: <\/u><\/b> Intermediate"
  },
  {
    "Id": "a0d78d2d260ff451e629aec329d50fbe",
    "resourceId": "Track 3",
    "title": "Bayesian modeling in R with {rstanarm}",
    "language": "Spanish",
    "instructors": "Zepeda Herrera, Fernando Antonio",
    "duration": 165,
    "attendees": 30,
    "level": "Intermediate",
    "start": "2021-07-07T17:45:00Z",
    "end": "2021-07-07T20:30:00Z",
    "summary": "This tutorial would introduce Bayesian modeling in R particularly \nthrough {rstanarm}. We would alternate between \"lectures\" and \n\"practical\" examples (with {learnr} tutorials). Starting with a brief \nintroduction of the Bayesian paradigm, we would cover linear and \ngeneralized linear regression as well as useful diagnostics and \nposterior visualization.",
    "description": "<b><u>Title<\/u><\/b> <br> Bayesian modeling in R with {rstanarm} <br><br><b><u>Summary<\/u><\/b> <br> This tutorial would introduce Bayesian modeling in R particularly \nthrough {rstanarm}. We would alternate between \"lectures\" and \n\"practical\" examples (with {learnr} tutorials). Starting with a brief \nintroduction of the Bayesian paradigm, we would cover linear and \ngeneralized linear regression as well as useful diagnostics and \nposterior visualization. <br><br><b><u>Instructors: <\/u><\/b> Zepeda Herrera, Fernando Antonio <br><b><u>Language: <\/u><\/b> Spanish <br><b><u>Level: <\/u><\/b> Intermediate"
  },
  {
    "Id": "c3269e149bc5b3bee7ed6ac1864a3b5b",
    "resourceId": "Track 3",
    "title": "Introduction to TileDB for R",
    "language": "English",
    "instructors": "Eddelbuettel, Dirk;\nWolen, Aaron",
    "duration": 180,
    "attendees": 200,
    "level": "Intermediate",
    "start": "2021-07-07T20:45:00Z",
    "end": "2021-07-07T23:45:00Z",
    "summary": "TileDB is an open source _universal data engine_ that natively supports \ndense and sparse multidimensional arrays, as well as data frames. Large \ndatasets can be stored on multiple backends ranging from a local \nfilesystem to cloud storage providers such as Amazon S3 (as well Google \nCloud Storage and Azure Cloud Storage) and accessed using almost any \nlanguage, including Python and R. The tutorial introduces the 'tiledb' R\n package on CRAN, which allows users to efficiently operate on large \ndense/sparse arrays using familiar R techniques and data structures. It \nalso offers key features of the underlying TileDB Embedded library: \nparallelised read and write operations, multiple compression formats, \ntime traveling (i.e., the ability to recover data stored at previous \ntimepoints), flexible encryption, and Apache Arrow support. Several \nsimple usage examples will be provided and you will have an \nopportunity to follow along on your laptops. One or two fuller usage \nexamples from Bioinformatics will serve as a more extended case study. \nWe will illustrate how TileDB can be used to create a performant data \nstore for results produced by Genome-Wide Association Studies, and \ndemonstrate the BioConductor package, TileDBArray, which is built on top\n of the DelayedArray framework and has shown excellent performance \nrelevant to existing (hdf5-based) solution. Finally, usage of TileDB \nwith cloud storage providers will be illustrated. This covers both \ndirect reads and writes to, for example, Amazon S3 as well as a brief \nillustration of the 'pay-as-you-go' Software-as-a-Service offering of \nTileDB Cloud with its additional features.",
    "description": "<b><u>Title<\/u><\/b> <br> Introduction to TileDB for R <br><br><b><u>Summary<\/u><\/b> <br> TileDB is an open source _universal data engine_ that natively supports \ndense and sparse multidimensional arrays, as well as data frames. Large \ndatasets can be stored on multiple backends ranging from a local \nfilesystem to cloud storage providers such as Amazon S3 (as well Google \nCloud Storage and Azure Cloud Storage) and accessed using almost any \nlanguage, including Python and R. The tutorial introduces the 'tiledb' R\n package on CRAN, which allows users to efficiently operate on large \ndense/sparse arrays using familiar R techniques and data structures. It \nalso offers key features of the underlying TileDB Embedded library: \nparallelised read and write operations, multiple compression formats, \ntime traveling (i.e., the ability to recover data stored at previous \ntimepoints), flexible encryption, and Apache Arrow support. Several \nsimple usage examples will be provided and you will have an \nopportunity to follow along on your laptops. One or two fuller usage \nexamples from Bioinformatics will serve as a more extended case study. \nWe will illustrate how TileDB can be used to create a performant data \nstore for results produced by Genome-Wide Association Studies, and \ndemonstrate the BioConductor package, TileDBArray, which is built on top\n of the DelayedArray framework and has shown excellent performance \nrelevant to existing (hdf5-based) solution. Finally, usage of TileDB \nwith cloud storage providers will be illustrated. This covers both \ndirect reads and writes to, for example, Amazon S3 as well as a brief \nillustration of the 'pay-as-you-go' Software-as-a-Service offering of \nTileDB Cloud with its additional features. <br><br><b><u>Instructors: <\/u><\/b> Eddelbuettel, Dirk;\nWolen, Aaron <br><b><u>Language: <\/u><\/b> English <br><b><u>Level: <\/u><\/b> Intermediate"
  },
  {
    "Id": "b8ebdd224dfcd5e68e952eaa4ea7d976",
    "resourceId": "Track 2",
    "title": "GET better at testing your R package!",
    "language": "English",
    "instructors": "Salmon, Maëlle;\nChamberlain, Scott",
    "duration": 180,
    "attendees": 50,
    "level": "Advanced",
    "start": "2021-07-07T07:00:00Z",
    "end": "2021-07-07T10:00:00Z",
    "summary": "Are you a package developer who wants to improve your understanding and practice of unit testing?\nYou've come to the right place: This tutorial is about Advanced testing of R packages, with HTTP testing as a case study.\nUnit tests have numerous advantages like \npreventing future breakage of your package and helping you define \nfeatures (test-driven development).\nIn many introductions to package development \nyou learn how to set up testthat infrastructure, and how to write a few \n“cute little tests” \n(https://testthat.r-lib.org/articles/test-fixtures.html#test-fixtures) \nwith only inline assertions.\nThis might work for a bit but soon you will \nencounter some practical and theoretical challenges: e.g. where do you \nput data and helpers for your tests? If your package is wrapping a web \nAPI, how do you test it independently from any internet connection? And \nhow do you test the behavior of your package in case of API errors?\nIn this tutorial we shall use HTTP testing \nwith the vcr package as an opportunity to empower you with more \nknowledge of testing principles (e.g. cleaning after yourself, testing \nerror behavior) and testthat practicalities (e.g. testthat helper files,\n testthat custom skippers).\nAfter this tutorial, you will be able to use \nthe handy vcr package for your package wrapping a web API or any other \nweb resource, but you will also have gained skills transferable to your \nother testing endeavours!\nCome and learn from rOpenSci expertise!\nRelated materials\nhttps://devguide.ropensci.org/building.html#testing\nhttps://books.ropensci.org/http-testing\nhttps://blog.r-hub.io/2019/10/29/mocking/\nhttps://blog.r-hub.io/2020/11/18/testthat-utility-belt/",
    "description": "<b><u>Title<\/u><\/b> <br> GET better at testing your R package! <br><br><b><u>Summary<\/u><\/b> <br> Are you a package developer who wants to improve your understanding and practice of unit testing?\nYou've come to the right place: This tutorial is about Advanced testing of R packages, with HTTP testing as a case study.\nUnit tests have numerous advantages like \npreventing future breakage of your package and helping you define \nfeatures (test-driven development).\nIn many introductions to package development \nyou learn how to set up testthat infrastructure, and how to write a few \n“cute little tests” \n(https://testthat.r-lib.org/articles/test-fixtures.html#test-fixtures) \nwith only inline assertions.\nThis might work for a bit but soon you will \nencounter some practical and theoretical challenges: e.g. where do you \nput data and helpers for your tests? If your package is wrapping a web \nAPI, how do you test it independently from any internet connection? And \nhow do you test the behavior of your package in case of API errors?\nIn this tutorial we shall use HTTP testing \nwith the vcr package as an opportunity to empower you with more \nknowledge of testing principles (e.g. cleaning after yourself, testing \nerror behavior) and testthat practicalities (e.g. testthat helper files,\n testthat custom skippers).\nAfter this tutorial, you will be able to use \nthe handy vcr package for your package wrapping a web API or any other \nweb resource, but you will also have gained skills transferable to your \nother testing endeavours!\nCome and learn from rOpenSci expertise!\nRelated materials\nhttps://devguide.ropensci.org/building.html#testing\nhttps://books.ropensci.org/http-testing\nhttps://blog.r-hub.io/2019/10/29/mocking/\nhttps://blog.r-hub.io/2020/11/18/testthat-utility-belt/ <br><br><b><u>Instructors: <\/u><\/b> Salmon, Maëlle;\nChamberlain, Scott <br><b><u>Language: <\/u><\/b> English <br><b><u>Level: <\/u><\/b> Advanced"
  },
  {
    "Id": "51e3851b8a8faaf87e03460993eded0f",
    "resourceId": "Track 2",
    "title": "Systematic data validation with the validate package",
    "language": "English",
    "instructors": "van der Loo, Mark;\nde Jonge, Edwin",
    "duration": 240,
    "attendees": 30,
    "level": "Intermediate",
    "start": "2021-07-07T10:15:00Z",
    "end": "2021-07-07T14:15:00Z",
    "summary": "Checking the quality of data is a task that pervades data analyses. It \ndoes not matter whether you are working with raw data, cleaned data, or \nwith the results of an analyses. It is always important to convince \nyourself that the data you are using is fit for its intended purpose. \nSince it is such a common task, why not automate it? The 'validate' \npackage is designed for exactly this task: it implements a domain \nspecific language for data checking that aims to encompass any check you\n might wish to perform. In this course you will will learn to define and\n measure data quality in a precise way with the validate package. We \nwill focus on the main workflow, and show you how you can involve domain\n experts directly with your work, even if they do not know R. You will \nlearn the main principles of data validation, both from the point of \nview of organizing a data processing work flow, as well as from a more \nformal perspective. You will exercise data validation tasks that range \nfrom checking input format and types to complex checks that involve data\n from multiple sources. You will learn how to follow the evolution of \ndata quality as it is processed using the lumberjack package. And you \nwill learn how to flush out redundant or contradictory quality demands \nusing the validatetools package. The course will consist of hands-on \nwork, based on a prepared tutorial that will be published on GitHub. \nThere will be break-out sessions with assignments where you can discuss \nthe materials with other course participants. The presentations will \ninclude some Kahoot quizzes to keep things interactive, fun, and \nfocused.",
    "description": "<b><u>Title<\/u><\/b> <br> Systematic data validation with the validate package <br><br><b><u>Summary<\/u><\/b> <br> Checking the quality of data is a task that pervades data analyses. It \ndoes not matter whether you are working with raw data, cleaned data, or \nwith the results of an analyses. It is always important to convince \nyourself that the data you are using is fit for its intended purpose. \nSince it is such a common task, why not automate it? The 'validate' \npackage is designed for exactly this task: it implements a domain \nspecific language for data checking that aims to encompass any check you\n might wish to perform. In this course you will will learn to define and\n measure data quality in a precise way with the validate package. We \nwill focus on the main workflow, and show you how you can involve domain\n experts directly with your work, even if they do not know R. You will \nlearn the main principles of data validation, both from the point of \nview of organizing a data processing work flow, as well as from a more \nformal perspective. You will exercise data validation tasks that range \nfrom checking input format and types to complex checks that involve data\n from multiple sources. You will learn how to follow the evolution of \ndata quality as it is processed using the lumberjack package. And you \nwill learn how to flush out redundant or contradictory quality demands \nusing the validatetools package. The course will consist of hands-on \nwork, based on a prepared tutorial that will be published on GitHub. \nThere will be break-out sessions with assignments where you can discuss \nthe materials with other course participants. The presentations will \ninclude some Kahoot quizzes to keep things interactive, fun, and \nfocused. <br><br><b><u>Instructors: <\/u><\/b> van der Loo, Mark;\nde Jonge, Edwin <br><b><u>Language: <\/u><\/b> English <br><b><u>Level: <\/u><\/b> Intermediate"
  },
  {
    "Id": "82c592fb201bad6034bfab58b5d4a655",
    "resourceId": "Track 2",
    "title": "Production-grade Shiny Apps with {golem}",
    "language": "French",
    "instructors": "Guyader, Vincent;\nGirard, Cervan",
    "duration": 210,
    "attendees": 30,
    "level": "Intermediate",
    "start": "2021-07-07T14:30:00Z",
    "end": "2021-07-07T18:00:00Z",
    "summary": "This tutorial is aimed at intermediate or advanced shiny application \ndevelopers who want to design \"clean\" applications following best \npractices. We will present the different steps necessary to obtain an \napplication deployed in production. An active participation of the \nparticipants is expected, with screen sharing, microphone (and if \npossible webcam).",
    "description": "<b><u>Title<\/u><\/b> <br> Production-grade Shiny Apps with {golem} <br><br><b><u>Summary<\/u><\/b> <br> This tutorial is aimed at intermediate or advanced shiny application \ndevelopers who want to design \"clean\" applications following best \npractices. We will present the different steps necessary to obtain an \napplication deployed in production. An active participation of the \nparticipants is expected, with screen sharing, microphone (and if \npossible webcam). <br><br><b><u>Instructors: <\/u><\/b> Guyader, Vincent;\nGirard, Cervan <br><b><u>Language: <\/u><\/b> French <br><b><u>Level: <\/u><\/b> Intermediate"
  },
  {
    "Id": "45dfd491842988c49f39ce7d84167bd9",
    "resourceId": "Track 2",
    "title": "Penguins in a Box: Interactive Data Science Tutorial with Penguins.",
    "language": "English",
    "instructors": "Dermit, Maria;\nEscobar, Susana",
    "duration": 60,
    "attendees": 100,
    "level": "Intermediate",
    "start": "2021-07-07T18:15:00Z",
    "end": "2021-07-07T19:15:00Z",
    "summary": "Penguins in a Box is a learnr package that covers the topics of R for Data Science book and uses the widely used dataset penguins to\n explore book's concepts. The package currently contains one tutorial \nfor each chapter of the book and will be introduced during the \npresentation. In addition, you will join breakout rooms to work on\nmodules on the book's main sections (e.i. Explore, Wrangle, Program,\n Model and Communicate; 6 sections in total) according to your learning\n objectives. This tutorial is aimed at both students \nwho want to improve their data science skills in an interactive way and \nteachers who want to access additional learnr resources similar to \nRstudio Primers (https://rstudio.cloud/learn/primers). The tutorial is \naimed to be interactive and peer-instruction between attendees is aimed \nto guide learning at breakout rooms.",
    "description": "<b><u>Title<\/u><\/b> <br> Penguins in a Box: Interactive Data Science Tutorial with Penguins. <br><br><b><u>Summary<\/u><\/b> <br> Penguins in a Box is a learnr package that covers the topics of R for Data Science book and uses the widely used dataset penguins to\n explore book's concepts. The package currently contains one tutorial \nfor each chapter of the book and will be introduced during the \npresentation. In addition, you will join breakout rooms to work on\nmodules on the book's main sections (e.i. Explore, Wrangle, Program,\n Model and Communicate; 6 sections in total) according to your learning\n objectives. This tutorial is aimed at both students \nwho want to improve their data science skills in an interactive way and \nteachers who want to access additional learnr resources similar to \nRstudio Primers (https://rstudio.cloud/learn/primers). The tutorial is \naimed to be interactive and peer-instruction between attendees is aimed \nto guide learning at breakout rooms. <br><br><b><u>Instructors: <\/u><\/b> Dermit, Maria;\nEscobar, Susana <br><b><u>Language: <\/u><\/b> English <br><b><u>Level: <\/u><\/b> Intermediate"
  },
  {
    "Id": "b2130e5c6a8f58f249f56ca2c4917d84",
    "resourceId": "Track 2",
    "title": "Professional, Polished, Presentable: Making Great Slides with xaringan",
    "language": "English",
    "instructors": "Aden-Buie, Garrick;\nCanelón, Silvia",
    "duration": 180,
    "attendees": 60,
    "level": "Intermediate",
    "start": "2021-07-07T20:00:00Z",
    "end": "2021-07-07T23:00:00Z",
    "summary": "The xaringan package brings professional, impressive, and visually \nappealing slides to the powerful R Markdown ecosystem. Through our \nhands-on tutorial, you will learn how to design highly \neffective slides that support presentations for teaching and reporting \nalike. Over three hours, you will learn how to create an \naccessible baseline design that matches your institution or \norganization’s style guide. Together we’ll explore the basics of CSS—the\n design language of the internet—and how we can leverage CSS to produce \nelegant slides for effective communication. Finally, we’ll deploy our \nslides online where they can be shared and discovered by others long \nafter they support our presentations. The tutorial will \ndemonstrate how to use the skills learned to incorporate\n principles of accessible design into their presentations. The tutorial \nwill feature live-coding and interactive question and answer periods, \ninterspersed with small-group break out sessions for \nguided hands-on experience. The tutorial will be supported by a \nrepository of materials.",
    "description": "<b><u>Title<\/u><\/b> <br> Professional, Polished, Presentable: Making Great Slides with xaringan <br><br><b><u>Summary<\/u><\/b> <br> The xaringan package brings professional, impressive, and visually \nappealing slides to the powerful R Markdown ecosystem. Through our \nhands-on tutorial, you will learn how to design highly \neffective slides that support presentations for teaching and reporting \nalike. Over three hours, you will learn how to create an \naccessible baseline design that matches your institution or \norganization’s style guide. Together we’ll explore the basics of CSS—the\n design language of the internet—and how we can leverage CSS to produce \nelegant slides for effective communication. Finally, we’ll deploy our \nslides online where they can be shared and discovered by others long \nafter they support our presentations. The tutorial will \ndemonstrate how to use the skills learned to incorporate\n principles of accessible design into their presentations. The tutorial \nwill feature live-coding and interactive question and answer periods, \ninterspersed with small-group break out sessions for \nguided hands-on experience. The tutorial will be supported by a \nrepository of materials. <br><br><b><u>Instructors: <\/u><\/b> Aden-Buie, Garrick;\nCanelón, Silvia <br><b><u>Language: <\/u><\/b> English <br><b><u>Level: <\/u><\/b> Intermediate"
  },
  {
    "Id": "X1",
    "resourceId": "Track 2",
    "title": "Translating R to Your Language",
    "language": "English",
    "instructors": "Michael Chirico, Michael Lawrence",
    "duration": 180,
    "attendees": 30,
    "level": "Intermediate",
    "start": "2021-07-07T23:15:00Z",
    "end": "2021-07-08T02:15:00Z",
    "summary": "R users are a global bunch. Providing error messages in languages besides\nEnglish can greatly improve the user experience (and debugging experience) of\nthose using R who may not be English natives.\nThis tutorial aims to get package developers \nand other R community members started implementing foreign-language \ntranslations of R's messages (errors, warnings, verbose output, etc.) \ninto a language of their choosing.\nThe standard tools for providing translations can be somewhat esoteric; in this\ntutorial, we'll go over some of the challenges presented by translations, the\nprocess for providing and/or updating translations to R itself, and finally\nintroduce a package (`potools`) that will remove some of the frictions\npotential translators may face.\nWe especially encourage attendance from speakers of major world languages\ncurrently missing from the R translation data base, in particular Hindi, Arabic,\nBengali, Urdu, and Bahasa Indonesia.",
    "description": "<b><u>Title<\/u><\/b> <br> Translating R to Your Language <br><br><b><u>Summary<\/u><\/b> <br> R users are a global bunch. Providing error messages in languages besides\nEnglish can greatly improve the user experience (and debugging experience) of\nthose using R who may not be English natives.\nThis tutorial aims to get package developers \nand other R community members started implementing foreign-language \ntranslations of R's messages (errors, warnings, verbose output, etc.) \ninto a language of their choosing.\nThe standard tools for providing translations can be somewhat esoteric; in this\ntutorial, we'll go over some of the challenges presented by translations, the\nprocess for providing and/or updating translations to R itself, and finally\nintroduce a package (`potools`) that will remove some of the frictions\npotential translators may face.\nWe especially encourage attendance from speakers of major world languages\ncurrently missing from the R translation data base, in particular Hindi, Arabic,\nBengali, Urdu, and Bahasa Indonesia. <br><br><b><u>Instructors: <\/u><\/b> Michael Chirico, Michael Lawrence <br><b><u>Language: <\/u><\/b> English <br><b><u>Level: <\/u><\/b> Intermediate"
  }
]
